[
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "The inductive bias of relu networks on orthogonally separable data",
    "authors": [
      {
        "family": "Phuong",
        "given": "Mary"
      },
      {
        "family": "Lampert",
        "given": "Christoph H."
      }
    ],
    "journal": "International Conference on Learning Representations",
    "year": 2021,
    "arxiv": "",
    "doi": "",
    "description": "Prove that the neurons of two-layer ReLU neural networks will asymptotically converge to two directions (align in two directions): the positive max-margin vector and the negative max-margin vector on orthogonally separable data.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs",
    "authors": [
      {
        "family": "Boursier",
        "given": "Etienne"
      },
      {
        "family": "Pillaud-Vivien",
        "given": "Loucas"
      },
      {
        "family": "Flammarion",
        "given": "Nicolas"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "arxiv": "2206.00939",
    "doi": "",
    "description": "Prove that, for orthogonal input vectors, the gradient flow dynamics of training two-layer ReLU neural networks on the mean squared error loss exhibit an initial alignment phase, and estimate the timescale on which this phase occurs.",
    "supplementary": ""
  },
    {
    "type": "conference",
    "tags": ["theory"],
    "title": "Towards Understanding the Condensation of Neural Networks at Initial Training",
    "authors": [
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Zhou",
        "given": "Qixuan"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "arxiv": "2105.11686",
    "doi": "",
    "description": "This work investigates the phenomenon of neural network condensation during initial training, characterizing the maximal number of orientations based on the activation function's multiplicity.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width",
    "authors": [
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Zhou",
        "given": "Qixuan"
      },
      {
        "family": "Jin",
        "given": "Zhenyuan"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "arxiv": "2205.12101",
    "doi": "",
    "description": "This work presents an empirical phase diagram (condense, critical and linear regimes) for three-layer neural networks with infinite width, revealing distinct phases of different layers.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Learning a neuron by a shallow relu network: Dynamics and implicit bias for correlated inputs",
    "authors": [
      {
        "family": "Chistikov",
        "given": "Dmitry"
      },
      {
        "family": "Englert",
        "given": "Matthias"
      },
      {
        "family": "Lazic",
        "given": "Ranko"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2023,
    "arxiv": "2306.06479",
    "doi": "",
    "description": "This work shows that when use two-layer ReLU neural networks to learn a target function of one neuron with correlated inputs, the neurons will first align and will not separate during training.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks",
    "authors": [
      {
        "family": "Wang",
        "given": "Mingze"
      },
      {
        "family": "Ma",
        "given": "Chao"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2023,
    "arxiv": "2305.12467",
    "doi": "",
    "description": "This paper estimates the time of early alignment phase in the binary classification problem of effectively two data points, which are separated by small angle.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Early neuron alignment in two-layer relu networks with small initialization",
    "authors": [
      {
        "family": "Min",
        "given": "Hancheng"
      },
      {
        "family": "Mallada",
        "given": "Enrique"
      },
      {
        "family": "Vidal",
        "given": "Rene"
      }
    ],
    "journal": "International Conference on Learning Representations",
    "year": 2024,
    "arxiv": "2307.12851",
    "doi": "",
    "description": "This paper looses the data assumption to that the data are positively correlated when they have the same labels and estimate the timescale of early alignment.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Early alignment in two-layer networks training is a two-edged sword",
    "authors": [
      {
        "family": "Boursier",
        "given": "Etienne"
      },
      {
        "family": "Flammarion",
        "given": "Nicolas"
      }
    ],
    "journal": "Journal of Machine Learning Research",
    "year": 2025,
    "arxiv": "2401.10791",
    "doi": "",
    "description": "",
    "supplementary": "two-layer-two-edge.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Directional convergence near small initializations and saddles in two-homogeneous neural networks",
    "authors": [
      {
        "family": "Kumar",
        "given": "Akshay"
      },
      {
        "family": "Haupt",
        "given": "Jarvis"
      }
    ],
    "journal": "Transactions on Machine Learning Research",
    "year": 2024,
    "arxiv": "2402.09226",
    "doi": "",
    "description": "This paper analyzes the alignment near small initializations and saddle points on two-homogeneous neural networks.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Early directional convergence in deep homogeneous neural networks for small initializations",
    "authors": [
      {
        "family": "Kumar",
        "given": "Akshay"
      },
      {
        "family": "Haupt",
        "given": "Jarvis"
      }
    ],
    "journal": "Transactions on Machine Learning Research",
    "year": 2025,
    "arxiv": "2403.08121",
    "doi": "",
    "description": "This paper analyzes the alignment near small initializations on deep homogeneous neural networks.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Phase diagram of initial condensation for two-layer neural networks",
    "authors": [
      {
        "family": "Chen",
        "given": "Zhengan"
      },
      {
        "family": "Li",
        "given": "Yuqing"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhou",
        "given": "Zhangchen"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "CSIAM Trans. Appl. Math",
    "year": 2024,
    "arxiv": "2303.06561",
    "doi": "10.4208/csiam-am.so-2023-0016",
    "description": "",
    "supplementary": "initial-condense.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Understanding the initial condensation of convolutional neural networks",
    "authors": [
      {
        "family": "Zhou",
        "given": "Zhangchen"
      },
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Li",
        "given": "Yuqing"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "CSIAM Trans. Appl. Math",
    "year": 2025,
    "arxiv": "2305.09947",
    "doi": "10.4208/csiam-am.so-2024-0011",
    "description": "",
    "supplementary": "initial-condense-conv.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Implicit regularization of dropout",
    "authors": [
      {
        "family": "Zhang",
        "given": "Zhongwang"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2024,
    "arxiv": "2207.05952",
    "doi": "10.1109/tpami.2024.3357172",
    "description": "This paper shows that dropout can facilitate condensation even without small initialization.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["algorithm"],
    "title": "Efficient and flexible method for reducing moderate-size deep neural networks with condensation",
    "authors": [
      {
        "family": "Chen",
        "given": "Tianyi"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Entropy",
    "year": 2024,
    "arxiv": "2405.01041",
    "doi": "10.3390/e26070567",
    "description": "This paper employs a condensation-based reduction method to compress the network size across diverse tasks.",
    "supplementary": ""
  }
]