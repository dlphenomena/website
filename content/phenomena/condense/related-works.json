[
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "The inductive bias of relu networks on orthogonally separable data",
    "authors": [
      {
        "family": "Phuong",
        "given": "Mary"
      },
      {
        "family": "Lampert",
        "given": "Christoph H."
      }
    ],
    "journal": "International Conference on Learning Representations",
    "year": 2021,
    "url": "https://openreview.net/forum?id=krz7T0xU9Z_",
    "pdfLink": "https://openreview.net/pdf?id=krz7T0xU9Z_",
    "description": "Prove that the neurons of two-layer ReLU neural networks will asymptotically converge to two directions (align in two directions): the positive max-margin vector and the negative max-margin vector on orthogonally separable data.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs",
    "authors": [
      {
        "family": "Boursier",
        "given": "Etienne"
      },
      {
        "family": "Pillaud-Vivien",
        "given": "Loucas"
      },
      {
        "family": "Flammarion",
        "given": "Nicolas"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/7eeb9af3eb1f48e29c05e8dd3342b286-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2022/file/7eeb9af3eb1f48e29c05e8dd3342b286-Paper-Conference.pdf",
    "arxiv": "2206.00939",
    "doi": "",
    "description": "Prove that, for orthogonal input vectors, the gradient flow dynamics of training two-layer ReLU neural networks on the mean squared error loss exhibit an initial alignment phase, and estimate the timescale on which this phase occurs.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Towards Understanding the Condensation of Neural Networks at Initial Training",
    "authors": [
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Zhou",
        "given": "Qixuan"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/0f4d1fc085b7504c140e66bb26ed8842-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2022/file/0f4d1fc085b7504c140e66bb26ed8842-Paper-Conference.pdf",
    "arxiv": "2105.11686",
    "doi": "",
    "description": "This work investigates the phenomenon of neural network condensation during initial training, characterizing the maximal number of orientations based on the activation function's multiplicity.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width",
    "authors": [
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Zhou",
        "given": "Qixuan"
      },
      {
        "family": "Jin",
        "given": "Zhenyuan"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2022,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a71c1931d3fb8ba564f7458d0657d0b1-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2022/file/a71c1931d3fb8ba564f7458d0657d0b1-Paper-Conference.pdf",
    "arxiv": "2205.12101",
    "doi": "",
    "description": "This work presents an empirical phase diagram (condense, critical and linear regimes) for three-layer neural networks with infinite width, revealing distinct phases of different layers.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Learning a neuron by a shallow relu network: Dynamics and implicit bias for correlated inputs",
    "authors": [
      {
        "family": "Chistikov",
        "given": "Dmitry"
      },
      {
        "family": "Englert",
        "given": "Matthias"
      },
      {
        "family": "Lazic",
        "given": "Ranko"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2023,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/4af24e6ce753c181e703f3f0be3b5e20-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2023/file/4af24e6ce753c181e703f3f0be3b5e20-Paper-Conference.pdf",
    "arxiv": "2306.06479",
    "doi": "",
    "description": "This work shows that when use two-layer ReLU neural networks to learn a target function of one neuron with correlated inputs, the neurons will first align and will not separate during training.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks",
    "authors": [
      {
        "family": "Wang",
        "given": "Mingze"
      },
      {
        "family": "Ma",
        "given": "Chao"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2023,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7016d7b7b6e3c05b2128ac5b3aae492d-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7016d7b7b6e3c05b2128ac5b3aae492d-Paper-Conference.pdf",
    "arxiv": "2305.12467",
    "doi": "",
    "description": "This paper estimates the time of early alignment phase in the binary classification problem of effectively two data points, which are separated by small angle.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing",
    "authors": [
      {
        "family": "Zhang",
        "given": "Zhongwang"
      },
      {
        "family": "Lin",
        "given": "Pengxiao"
      },
      {
        "family": "Wang",
        "given": "Zhiwei"
      },
      {
        "family":"Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Advances in Neural Information Processing Systems",
    "year": 2024,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/19c145aaad40927c51f4d10eaa339c20-Abstract-Conference.html",
    "pdfLink": "https://proceedings.neurips.cc/paper_files/paper/2024/file/19c145aaad40927c51f4d10eaa339c20-Paper-Conference.pdf",
    "arxiv": "2405.05409",
    "doi": "",
    "description": "This paper reveals that initialization is critical to the reasoning ability of transformers, and demonstrates that this criticality is strongly correlated with condensation.",
    "supplementary": ""
  },
  {
    "type": "conference",
    "tags": ["theory"],
    "title": "Early neuron alignment in two-layer relu networks with small initialization",
    "authors": [
      {
        "family": "Min",
        "given": "Hancheng"
      },
      {
        "family": "Mallada",
        "given": "Enrique"
      },
      {
        "family": "Vidal",
        "given": "Rene"
      }
    ],
    "journal": "International Conference on Learning Representations",
    "year": 2024,
    "url": "https://openreview.net/forum?id=QibPzdVrRu",
    "pdfLink": "https://openreview.net/pdf?id=QibPzdVrRu",
    "arxiv": "2307.12851",
    "doi": "",
    "description": "This paper looses the data assumption to that the data are positively correlated when they have the same labels and estimate the timescale of early alignment.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Early alignment in two-layer networks training is a two-edged sword",
    "authors": [
      {
        "family": "Boursier",
        "given": "Etienne"
      },
      {
        "family": "Flammarion",
        "given": "Nicolas"
      }
    ],
    "journal": "Journal of Machine Learning Research",
    "year": 2025,
    "url": "https://www.jmlr.org/papers/v26/24-1523.html",
    "pdfLink": "https://www.jmlr.org/papers/volume26/24-1523/24-1523.pdf",
    "arxiv": "2401.10791",
    "doi": "",
    "description": "",
    "supplementary": "two-layer-two-edge.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Directional convergence near small initializations and saddles in two-homogeneous neural networks",
    "authors": [
      {
        "family": "Kumar",
        "given": "Akshay"
      },
      {
        "family": "Haupt",
        "given": "Jarvis"
      }
    ],
    "journal": "Transactions on Machine Learning Research",
    "year": 2024,
    "url": "https://openreview.net/forum?id=hfrPag75Y0",
    "pdfLink": "https://openreview.net/pdf?id=hfrPag75Y0",
    "arxiv": "2402.09226",
    "doi": "",
    "description": "This paper analyzes the alignment near small initializations and saddle points on two-homogeneous neural networks.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Early directional convergence in deep homogeneous neural networks for small initializations",
    "authors": [
      {
        "family": "Kumar",
        "given": "Akshay"
      },
      {
        "family": "Haupt",
        "given": "Jarvis"
      }
    ],
    "journal": "Transactions on Machine Learning Research",
    "year": 2025,
    "url": "https://openreview.net/forum?id=VNM6V1gi3k",
    "pdfLink": "https://openreview.net/pdf?id=VNM6V1gi3k",
    "arxiv": "2403.08121",
    "doi": "",
    "description": "This paper analyzes the alignment near small initializations on deep homogeneous neural networks.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Phase diagram of initial condensation for two-layer neural networks",
    "authors": [
      {
        "family": "Chen",
        "given": "Zhengan"
      },
      {
        "family": "Li",
        "given": "Yuqing"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhou",
        "given": "Zhangchen"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "CSIAM Trans. Appl. Math",
    "year": 2024,
    "arxiv": "2303.06561",
    "doi": "10.4208/csiam-am.so-2023-0016",
    "description": "",
    "supplementary": "initial-condense.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Understanding the initial condensation of convolutional neural networks",
    "authors": [
      {
        "family": "Zhou",
        "given": "Zhangchen"
      },
      {
        "family": "Zhou",
        "given": "Hanxu"
      },
      {
        "family": "Li",
        "given": "Yuqing"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "CSIAM Trans. Appl. Math",
    "year": 2025,
    "arxiv": "2305.09947",
    "doi": "10.4208/csiam-am.so-2024-0011",
    "description": "",
    "supplementary": "initial-condense-conv.md"
  },
  {
    "type": "journal",
    "tags": ["theory"],
    "title": "Implicit regularization of dropout",
    "authors": [
      {
        "family": "Zhang",
        "given": "Zhongwang"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2024,
    "arxiv": "2207.05952",
    "doi": "10.1109/tpami.2024.3357172",
    "description": "This paper shows that dropout can facilitate condensation even without small initialization.",
    "supplementary": ""
  },
  {
    "type": "journal",
    "tags": ["algorithm"],
    "title": "Efficient and flexible method for reducing moderate-size deep neural networks with condensation",
    "authors": [
      {
        "family": "Chen",
        "given": "Tianyi"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "journal": "Entropy",
    "year": 2024,
    "arxiv": "2405.01041",
    "doi": "10.3390/e26070567",
    "description": "This paper employs a condensation-based reduction method to compress the network size across diverse tasks.",
    "supplementary": ""
  }
]
